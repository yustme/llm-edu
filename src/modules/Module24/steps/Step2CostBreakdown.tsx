import { InfoPanel } from "@/components/presentation/InfoPanel";
import { InteractiveArea } from "@/components/presentation/InteractiveArea";
import { CostCalculator } from "@/components/optimization/CostCalculator";

export function Step2CostBreakdown() {
  return (
    <div className="flex gap-8">
      {/* Left: Info panel (~40%) */}
      <div className="w-2/5 shrink-0">
        <InfoPanel
          title="Where Costs Come From"
          highlights={["Input Tokens", "Output Tokens", "Model Choice"]}
        >
          <p>
            Every LLM API call has a cost determined by three main factors:
          </p>
          <ul className="list-disc space-y-1 pl-5">
            <li>
              <span className="font-medium">Input tokens</span> - The system
              prompt, user message, and any context (RAG chunks, tool
              definitions) sent to the model
            </li>
            <li>
              <span className="font-medium">Output tokens</span> - The
              response generated by the model, including reasoning and final
              answer
            </li>
            <li>
              <span className="font-medium">Tool calls</span> - Each external
              tool invocation may have its own API cost
            </li>
          </ul>
          <p>
            The <strong>model selection</strong> has the largest impact on
            cost. GPT-4o costs 30x more per token than GPT-4o Mini, while
            Claude Sonnet costs 3x more than Claude Haiku.
          </p>
          <p>
            Use the calculator to explore how different configurations affect
            your per-query cost.
          </p>
        </InfoPanel>
      </div>

      {/* Right: Interactive area (~60%) */}
      <div className="flex-1">
        <InteractiveArea>
          <p className="mb-4 text-center text-sm font-medium text-muted-foreground">
            Cost Calculator
          </p>
          <CostCalculator />
        </InteractiveArea>
      </div>
    </div>
  );
}
